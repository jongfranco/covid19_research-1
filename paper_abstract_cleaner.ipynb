{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Covid-19 Research Paper Data Cleaning Pipeline\nIqra Munawar and Sameen Salam\n\n## Purpose\nTo take the abstracts from scientific literature related to Covid (and potentially any other academic field) and properly posture them for topic modeling and clustering. This particular text cleaner is fairly adapted for the realm of biology in that it seeks to preserve scientific terminology (i.e. capitalization of \"RNA\", hypenation of \"SP-D\", elimination of standalone numbers among other things) to make resulting models as accurate, interpretable, and convenient as possible.\n\n### Solved Items\n* Drops observations with no abstract or title  \n* Word tokenizes each abstract and eliminates common terms for abstracts specifically (BACKGROUND, CONCLUSIONS, etc.)  \n* Filters out regular stopwords based on NLTK corpus of stopwords  \n* Filters out all standard punctuation except for hyphens (\"-\")  \n* Eliminates standalone numbers (\"100\" or \"14\"), tokens that contain no letters (\"%3\" or \"35!\"), and tokens that are length one (\"C\" or \"f\")  \n* Filters out tokens that are two letters long and are not capital (\"sg\")  \n* Lowercasing all tokens that have a single capital letter and are 3 or more characters in length (\"Research\" to \"research\", but \"RNA\" remains intact)   \n* Removing specialty punctuation or characters that remain, outside of the standard punctuation (languages outside of alphanumeric characters)\n* Removing tokens with a hyphen at the end (\"corona-\" or \"noro-\")  \n* Lemmatization and final posturing of every token in each abstract  \n\n### Pending Items\n* Cannot eliminate abstracts in languages that share the same alphabet (German or Spanish)\n* Code cannot be run on full dataset (~104K abstracts as of this version) due to computational constraints (Kaggle server limit of ~9 hrs)\n* spaCy lemmatizer does not always work in cases that should not be ambiguous (\"detected\" to \"detect\") "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Loading in the necessary libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\nimport spacy\nimport string\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#       print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data source comes from CORD-19 research challenge on Kaggle. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading in the metadata (for the abstracts) and creating a copy so I don't have to constantly reload this data.\nall_source_metadata = pd.read_csv(\"/kaggle/input/CORD-19-research-challenge/metadata.csv\")\nmetadata = all_source_metadata.copy()","execution_count":3,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n  interactivity=interactivity, compiler=compiler, result=result)\n","name":"stderr"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Subsetting for observations with actual text in the abstract column\n#Then I reset the index and dropped the old index column...I forgot why\nmetadata = metadata[metadata[\"abstract\"].notnull()]\nmetadata = metadata.reset_index(drop = True)\n\nmetadata = metadata[metadata[\"title\"].notnull()]\nmetadata = metadata.reset_index(drop = True)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get shape of metadata \nmetadata.shape","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"(104393, 19)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting a subset of the data for processing because of computational limitations\nreasonable = metadata.sample(n=75000,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below we are defining the variables that need to be specified once for the function to work properly. We use regular English stopwords and added some additional stopwords that appeared very frequently in the data but contributed very little scientific value. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining English Stopwords and other additional stopwords\nstop_words=set(stopwords.words(\"english\"))\nspecial_stop_words = [\"BACKGROUND\", \"METHODS\", \"CONCLUSION\", \"RESULTS\", \":\", \"Abstract\", \"abstract\", \"ABSTRACT\", \"CONCLUSIONS\",\\\n                          \"SUPPLEMENTARY\", \"MATERIAL\", \"OBJECTIVE\", \"IMPORTANCE\", \"METHODOLOGY\", \"METHODOLOGYPRINCIPAL\", \"DESIGN\",\\\n                          \"Background\", \"PURPOSE\", \"MATERIALS\", \"INTRODUCTION\", \"ELECTRONIC\"]\n    \n# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n#Defining the punctuation for this project (again, can be done outside of this function)\nspecial_punc = [x for i,x in enumerate(string.punctuation) if x!= \"-\"]\n\n#Defining a porter stemmer object\n#ps = PorterStemmer()","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is the function doc_proc that does the cleaning."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a function to clean all abstracts: doc_proc\ndef doc_proc(text):\n    \n    '''\n    doc_proc is a function that takes in a text item (in this case, a scientific journal abstract) and conducts all pre-processing before \n    further analysis. The function automatically does all of the above individual preprocessing steps. This function outputs an object after\n    word lemmatization (lemmatized_words). This can be applied over a dataframe through the apply functions.\n    \n    Keyword Arguments:\n    text: the input text to be processed. Must be a continuous string that can be tokenized into words.\n    \n    '''\n    \n    ###Starting the text manipulation process###\n    \n    #Tokenizing into words\n    tokenized_word=word_tokenize(text)\n\n    #Filtering out special stopwords first and rejoining the words back together into sentences\n    special_filtered_word=[]\n    for w in tokenized_word:\n        if w not in special_stop_words:\n            special_filtered_word.append(w)   \n    s = \" \"\n    special_filtered_word = s.join(special_filtered_word)\n   \n    #Word Tokenizing all text\n    text = word_tokenize(special_filtered_word)\n    \n    #Filtering out all stopwords\n    stopword_filtered = []\n    for w in text:\n        if w.lower() not in stop_words:\n            stopword_filtered.append(w)\n    \n    \n    #Filtering out all punctuation, then rejoining all of the resulting sublists into one list\n    punc_filtered = [''.join(c for c in s if c not in special_punc) for s in stopword_filtered]\n    punc_filtered = [s for s in punc_filtered if s]\n    \n    #Filtering out stand-alone numbers\n    numeric_filtered = [term for term in punc_filtered if term.isdecimal() == False]\n    \n    #Filtering out any tokens that contain no letters\n    numeric_filtered2 = [term for term in numeric_filtered if re.search('[a-zA-Z]', term) is not None]\n    \n    #Filtering out any tokens left over that are length one (i.e. \"C\" or \"f\")\n    single_stripped = [term for term in numeric_filtered2 if len(term) > 1]\n    \n    #Filtering out any tokens left two letters long and lower case (i.e. \"sg\")\n    lower_double_stripped = [term for term in single_stripped if len(term) is not 2 or term.isupper() == True]\n    \n    #Lower casing all tokens that have a single capital letter and are longer than 2 characters long\n    lower_cased_nonsci = []\n    for term in lower_double_stripped:\n        if len(term) > 2 and sum(1 for c in term if c.isupper()) <= 1:\n            lower_cased_nonsci.append(term.lower())\n        else:\n            lower_cased_nonsci.append(term)\n    \n    #Removing all specialty punctuation outside of default ones\n    for i in range(len(lower_cased_nonsci)):\n        if len(lower_cased_nonsci[i]) == 1 and lower_cased_nonsci[i].lower() not in list(string.ascii_lowercase):\n            lower_cased_nonsci[i] = ''        \n    while('' in lower_cased_nonsci) : \n        lower_cased_nonsci.remove('') \n    \n    #Removing all tokens that end in a hyphen\n    suffix_hyphen_stripped = [term for term in lower_cased_nonsci if term.endswith(\"-\") == False]\n    \n    #STEMMING METHOD####################################################################\n    # Parse the sentence using the loaded 'en' model object `nlp`\n    #final_words = []\n    #for term in suffix_hyphen_stripped:\n    #    final_words.append(ps.stem(term))\n    ####################################################################################\n    #LEMMATIZATION METHOD###############################################################\n    #\n    final_words = []\n    for term in suffix_hyphen_stripped:\n        doc = nlp(term)\n        final_words.append([token.lemma_ for token in doc])\n    \n    #Recombining sublists containing more than one token\n    recombined1 = []\n    for term_list in final_words:\n        if len(term_list) > 1:\n            recombined1.append([\"\".join(term_list)])\n        else:\n            recombined1.append(term_list)\n    \n    #Recombining all sublists into a main list\n    recombined2 = [item for sublist in recombined1 for item in sublist]\n    #####################################################################################        \n    return(recombined2)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code to test run times for different versions of doc_proc\n\n#import time\n#start_time = time.time()\n#test = doc_proc(metadata[\"abstract\"][18])\n#end_time = time.time()\n\n#end_time-start_time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying doc_proc to the small dataset into a new column \"abstract2\" \nreasonable[\"abstract2\"] = reasonable['abstract'].apply(doc_proc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following output shows a comparison between the original and cleaned forms of a fairly messy abstract in the data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the results\nprint(metadata[\"abstract\"][0])\nprint('----------------------------------------------')\nprint(doc_proc(metadata[\"abstract\"][0]))","execution_count":12,"outputs":[{"output_type":"stream","text":"OBJECTIVE: This retrospective chart review describes the epidemiology and clinical features of 40 patients with culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia. METHODS: Patients with positive M. pneumoniae cultures from respiratory specimens from January 1997 through December 1998 were identified through the Microbiology records. Charts of patients were reviewed. RESULTS: 40 patients were identified, 33 (82.5%) of whom required admission. Most infections (92.5%) were community-acquired. The infection affected all age groups but was most common in infants (32.5%) and pre-school children (22.5%). It occurred year-round but was most common in the fall (35%) and spring (30%). More than three-quarters of patients (77.5%) had comorbidities. Twenty-four isolates (60%) were associated with pneumonia, 14 (35%) with upper respiratory tract infections, and 2 (5%) with bronchiolitis. Cough (82.5%), fever (75%), and malaise (58.8%) were the most common symptoms, and crepitations (60%), and wheezes (40%) were the most common signs. Most patients with pneumonia had crepitations (79.2%) but only 25% had bronchial breathing. Immunocompromised patients were more likely than non-immunocompromised patients to present with pneumonia (8/9 versus 16/31, P = 0.05). Of the 24 patients with pneumonia, 14 (58.3%) had uneventful recovery, 4 (16.7%) recovered following some complications, 3 (12.5%) died because of M pneumoniae infection, and 3 (12.5%) died due to underlying comorbidities. The 3 patients who died of M pneumoniae pneumonia had other comorbidities. CONCLUSION: our results were similar to published data except for the finding that infections were more common in infants and preschool children and that the mortality rate of pneumonia in patients with comorbidities was high.\n----------------------------------------------\n['retrospective', 'chart', 'review', 'describes', 'epidemiology', 'clinical', 'features', 'patient', 'culture-prove', 'mycoplasma', 'pneumoniae', 'infection', 'king', 'abdulaziz', 'university', 'hospital', 'jeddah', 'saudi', 'arabia', 'patient', 'positive', 'pneumoniae', 'culture', 'respiratory', 'specimen', 'january', 'december', 'identify', 'microbiology', 'record', 'charts', 'patient', 'review', 'patient', 'identify', 'require', 'admission', 'infection', 'community-acquire', 'infection', 'affected', 'age', 'group', 'common', 'infant', 'pre-school', 'child', 'occur', 'year-round', 'common', 'fall', 'spring', 'three-quarter', 'patient', 'comorbiditie', 'twenty-four', 'isolate', 'associated', 'pneumonia', 'upper', 'respiratory', 'tract', 'infection', 'bronchiolitis', 'cough', 'fever', 'malaise', 'common', 'symptom', 'crepitation', 'wheeze', 'common', 'sign', 'patient', 'pneumonia', 'crepitation', 'bronchial', 'breathe', 'immunocompromise', 'patient', 'likely', 'non-immunocompromised', 'patient', 'present', 'pneumonia', 'versus', 'patient', 'pneumonia', 'uneventful', 'recovery', 'recover', 'follow', 'complication', 'die', 'pneumoniae', 'infection', 'die', 'due', 'underlie', 'comorbiditie', 'patient', 'die', 'pneumoniae', 'pneumonia', 'comorbiditie', 'result', 'similar', 'publish', 'datum', 'except', 'finding', 'infection', 'common', 'infant', 'preschool', 'child', 'mortality', 'rate', 'pneumonia', 'patient', 'comorbiditie', 'high']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Exporting the data for use in other kernels for modeling\nreasonable.to_csv(\"abstract_cleaned.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}